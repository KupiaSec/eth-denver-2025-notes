# Impact Evaluation With Counterfactually

**Speakers:** Sejal Rekhan, Lau NaMu, Devansh Mehta, David Dao |


*Upload Date: 20250302*

*Source: [https://www.youtube.com/watch?v=K7oh8ZqJG2c](https://www.youtube.com/watch?v=K7oh8ZqJG2c)*

# Impact Evaluation With Counterfactuals - Sejal Rekhan et al.

This YouTube video, featuring Sejal Rekhan, Lau NaMu, Devansh Mehta, David Dao, discusses impact evaluation using counterfactual analysis, particularly in the context of web3 governance and funding.

## 1. Main Points

* **Counterfactual analysis is crucial for evaluating impact:**  The speakers highlight the importance of comparing the actual outcome of a project with a hypothetical outcome (counterfactual) in the absence of the project's intervention.
* **Defining impact is complex:**  The video emphasizes that defining and measuring impact is not straightforward and requires careful consideration of both outputs and outcomes.
* **Data limitations in web3:** The panelists acknowledged the challenges in collecting reliable data in the decentralized nature of web3, leading to limitations in precise impact assessment.
* **Examples from the Ethereum Foundation:** Real-world case studies from the Ethereum Foundation, particularly the short-term incentives program, are used to illustrate how counterfactual analysis can be applied in practice.  Examples include analyzing changes in the rate of deforestation or protocol activity.
* **Importance of considering existing metrics and external factors:** The discussion touches on the significance of examining pre-existing metrics and factors influencing the outcomes of interventions (e.g., what other projects were funded around the same time?).
* **Potential for gaming the system:**  The possibility of projects potentially "gaming" the metric system to appear more successful and attract funding is brought up.
* **Limitations of available data and models:**  The panelists discuss the limitations of current data sets and AI models used for impact evaluation, highlighting the need for further development to improve accuracy.
* **Importance of exploring innovative methods for impact evaluation:**  The discussion encourages exploring new approaches and models for impact evaluation and measurement beyond traditional outputs, considering a 'theory of change' framework.
* **The need for detailed and clear impact definitions:** This is crucial for accurately measuring the success of any project.
* **The role of a 'juror' in judgment:** The discussion touches upon using a 'juror' analogy to aid in judging the merit of projects.


## 2. Key Insights

The core insights revolve around the nuanced challenges inherent in evaluating the impact (or lack thereof) of web3 projects.  The panelists underscored the need for better-defined and measurable impact metrics.  They weren't simply discussing technical methods, but also the human elements of decision-making involved in funding and governance in the web3 space.  Key insights include:

* **Outputs vs. Outcomes:** The difference between delivering on a project's listed tasks (outputs) and the actual, measurable effect on the wider community (outcomes) is a critical distinction. Counterfactual analysis needs to consider both.
* **Data limitations in web3:**  Decentralization and diverse data sources make precise, quantitative impact analysis significantly more difficult compared to traditional environments.
* **Goodhart's Law:** The panel highlighted how rewarding specific metrics can often lead to distorted outcomes, as participants try to maximize those metrics rather than achieve real positive impact. This means that simply focusing on maximizing certain outcomes (like TVL) may not indicate true value.
* **The importance of contextual factors and historical analysis:** Understanding surrounding project circumstances and prior performance as part of impact studies.
* **Building a Dependency Graph of Impacts:** Panel members propose visual representation (dependency graph) of all elements of an ecosystem to understand the relationship of factors to outcomes.
* **The role of intuition and reasoned judgment:** While data is important, ultimately, judgment and intuition are vital components of a nuanced, well-rounded impact evaluation framework.

## 3. Practical Takeaways

* **Clearly Define and Measure Impact:**  Carefully define the intended outcomes and develop specific, measurable metrics to assess progress.
* **Go Beyond Outputs:**  Focus not only on the project's outputs, but also on the wider, community-level outcomes.
* **Consider Counterfactuals:**  Employ counterfactual analysis by comparing observed outcomes to what would have occurred without the project.
* **Use Historical Data:** Incorporate historical data of the ecosystem and similar projects to understand context and potential biases.
* **Explore New Metrics:**  Be open to developing new and innovative metrics to capture the unique nuances of web3 ecosystems.
* **Use Dependency Graphs:**  Create dependency graphs to visualize relationships between project actions and observed outcomes within the web3 ecosystem.
* **Incorporate Human Judgment:** Recognize that nuanced impact evaluations need to incorporate human judgment based on intuition, reasoning, and critical contextual understanding.

## 4. Additional Notes

The video demonstrates how complicated the problem of measuring impact is within decentralized ecosystems.  A key takeaway is the need for a holistic approach that considers both the quantifiable and the qualitative aspects of web3 project impact.  The use of AI tools and models was touched upon but was not the primary focus. While the speakers mentioned instances of data manipulation and protocol vulnerabilities causing negative outcomes, the overall tone leaned toward promoting a thoughtful and comprehensive approach to impact evaluation.